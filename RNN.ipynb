{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "strong-perception",
   "metadata": {},
   "source": [
    "### To load the saved model and run it on the test data, run the cells marked with #+  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "import collections\n",
    "from typing import List\n",
    "import numpy as np     \n",
    "from nltk.corpus import stopwords\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import setUpFunctions\n",
    "import evaluation\n",
    "nltk.download('stopwords')                         \n",
    "import matplotlib.pyplot as plt     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-speech",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+ unselect wanted language option\n",
    "language ='es'\n",
    "# language ='es'\n",
    "\n",
    "if(language == 'en'): \n",
    "    lang= 'English'\n",
    "else:\n",
    "    lang='Spanish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "train = setUpFunctions.getTrainTweets(language)\n",
    "test = setUpFunctions.getTestTweets(language)\n",
    "val = setUpFunctions.getTrialTweets(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "traintweets = train['tweet']\n",
    "trainlabels = train['emoji']\n",
    "valtweets = val['tweet']\n",
    "vallabels = val['emoji']  \n",
    "testtweets = test['tweet']\n",
    "testlabels = test['emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-blackjack",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "train_tweets = [tweet.strip().split() for tweet in traintweets] \n",
    "train_labels = [int(label.strip()) for label in trainlabels]\n",
    "test_tweets = [tweet.strip().split() for tweet in testtweets]\n",
    "test_labels = [int(label.strip()) for label in testlabels]\n",
    "val_tweets = [tweet.strip().split() for tweet in valtweets]\n",
    "val_labels = [int(label.strip()) for label in vallabels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-diesel",
   "metadata": {},
   "source": [
    "## Function to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-fighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "def cleanTweets(tweets: List[List[str]]) -> List[List[str]]:\n",
    "    cleanedTweets: List[List[str]] = []\n",
    "    for tweet in tweets:\n",
    "        cleanTweet: List[str] = ['START']\n",
    "        for token in tweet:\n",
    "            token = token.lower()\n",
    "            cleanTweet.append(token)\n",
    "        cleanedTweets.append(cleanTweet)\n",
    "    return cleanedTweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-pixel",
   "metadata": {},
   "source": [
    "## Function to extract vocabulary from the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "def extractVocab(tweets: List[List[str]], min_freq: int):\n",
    "    tokenFreq = collections.defaultdict(lambda: 0)\n",
    "    for tweet in tweets:\n",
    "        for token in tweet:\n",
    "            tokenFreq[token] += 1\n",
    "    vocab = sorted(tokenFreq.keys(), key=tokenFreq.get, reverse=True)\n",
    "\n",
    "    while tokenFreq[vocab[-1]] < min_freq:\n",
    "        vocab.pop()\n",
    "\n",
    "    vocab.remove('START')\n",
    "\n",
    "    return ['PAD', 'START', 'UNKNOWN'] + vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-colors",
   "metadata": {},
   "source": [
    "## Function to index the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "def indexTweets(tweets: List[List[str]]) -> List[List[int]]:\n",
    "    return [ [ token2index[token] for token in tweet ] for tweet in tweets ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-contemporary",
   "metadata": {},
   "source": [
    "## Function to pad the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "def padTweets(tweets: List[List[int]]) -> np.ndarray:\n",
    "    maxTweetLength = max(len(tweet) for tweet in tweets)\n",
    "    paddedTweets = np.zeros([len(tweets), maxTweetLength], np.int32)\n",
    "    for (i, tweet) in enumerate(tweets):\n",
    "        paddedTweets[i, :len(tweet)] = tweet\n",
    "\n",
    "    return paddedTweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-background",
   "metadata": {},
   "source": [
    "## Applying the above functions to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "cleanTrainTweets: List[List[str]] = cleanTweets(train_tweets)\n",
    "cleanValTweets: List[List[str]] = cleanTweets(val_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "vocab = extractVocab(cleanTrainTweets, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "token2index = collections.defaultdict(lambda: 2)\n",
    "for (index, token) in enumerate(vocab):\n",
    "    token2index[token] = index\n",
    "    \n",
    "train_tweets_index: List[List[int]] = indexTweets(cleanTrainTweets)\n",
    "val_tweets_index: List[List[int]] = indexTweets(cleanValTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "train_tweets_padded: np.ndarray = padTweets(train_tweets_index)\n",
    "val_tweets_padded: np.ndarray = padTweets(val_tweets_index)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-gamma",
   "metadata": {},
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SRNN = models.Sequential()\n",
    "model_SRNN.add(layers.Embedding(len(vocab), 64, input_length=None, mask_zero=True))  \n",
    "model_SRNN.add(layers.SimpleRNN(64, dropout=0.5))\n",
    "model_SRNN.add(layers.Dense(20, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SRNN.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy', \n",
    "                    metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_SRNN.fit(np.array(train_tweets_padded), np.array(train_labels),\n",
    "                          epochs=5, batch_size=32, \n",
    "                          validation_data=(np.array(val_tweets_padded), np.array(val_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-norway",
   "metadata": {},
   "source": [
    "## Saving the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SRNN.save('models/'+lang+'SRNNModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-butterfly",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM = models.Sequential()\n",
    "model_LSTM.add(layers.Embedding(len(vocab), 64, input_length=None, mask_zero=True))  \n",
    "model_LSTM.add(layers.LSTM(64, dropout=0.5))\n",
    "model_LSTM.add(layers.Dense(20, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_LSTM.fit(np.array(train_tweets_padded), np.array(train_labels),\n",
    "                          epochs=5, batch_size=32, \n",
    "                          validation_data=(np.array(val_tweets_padded), np.array(val_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-queen",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM.save('models/'+lang+'LSTMModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-elder",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU = models.Sequential()\n",
    "model_GRU.add(layers.Embedding(len(vocab), 64, input_length=None, mask_zero=True))  \n",
    "model_GRU.add(layers.GRU(64, dropout=0.5))\n",
    "model_GRU.add(layers.Dense(20, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_GRU.fit(np.array(train_tweets_padded), np.array(train_labels),\n",
    "                          epochs=5, batch_size=32, \n",
    "                          validation_data=(np.array(val_tweets_padded), np.array(val_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-winner",
   "metadata": {},
   "source": [
    "## Saving the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU.save('models/'+lang+'GRUModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-charger",
   "metadata": {},
   "source": [
    "## Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_BI_SRNN = models.Sequential()\n",
    "model_BI_SRNN.add(layers.Embedding(len(vocab), 64, input_length=None, mask_zero=True))  \n",
    "model_BI_SRNN.add(layers.Bidirectional(layers.SimpleRNN(64, dropout=0.5)))\n",
    "model_BI_SRNN.add(layers.Dense(20, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_BI_SRNN.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_BI_SRNN.fit(np.array(train_tweets_padded), np.array(train_labels),\n",
    "                          epochs=5, batch_size=32, \n",
    "                          validation_data=(np.array(val_tweets_padded), np.array(val_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-uniform",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_BI_SRNN.save('models/'+lang+'BidirectionalSRNNModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-sally",
   "metadata": {},
   "source": [
    "## LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_LSTM_RNN = models.Sequential()\n",
    "# model_LSTM_RNN.add(layers.Embedding(len(vocab), 64, input_length=None, mask_zero=True))  \n",
    "# model_LSTM_RNN.add(layers.LSTM(64, dropout=0.5, return_sequences=True))\n",
    "# model_LSTM_RNN.add(layers.SimpleRNN(32, dropout=0.5))\n",
    "# model_LSTM_RNN.add(layers.Dense(19, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_LSTM_RNN.compile(optimizer='adam',\n",
    "#                     loss='sparse_categorical_crossentropy',\n",
    "#                     metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model_LSTM_RNN.fit(np.array(train_tweets_padded), np.array(train_labels),\n",
    "#                           epochs=5, batch_size=32, \n",
    "#                           validation_data=(np.array(val_tweets_padded), np.array(val_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-dryer",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "#The path has to be changed according to the model that you would like to test \n",
    "model_RNN = load_model('models/'+lang+'LSTMModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_losses = history.history['loss']\n",
    "# val_losses = history.history['val_loss']\n",
    "# epochs = range(1, len(history.history['loss']) + 1)\n",
    "# plt.plot(epochs, train_losses, 'b-', label='Train loss')\n",
    "# plt.plot(epochs, val_losses, 'r-', label='Val loss')\n",
    "# plt.title('Train and val loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.ylim(0, 6)\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "def confusion_matrix(gold_labels, predicted):\n",
    "    classes = np.unique(gold_labels)\n",
    "\n",
    "    ## Plot confusion matrix\n",
    "    cm = metrics.confusion_matrix(gold_labels, predicted)\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)\n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-ferry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "test_tweets_clean: List[List[str]] = cleanTweets(test_tweets)\n",
    "test_tweets_index: List[List[int]] = indexTweets(test_tweets_clean)\n",
    "test_tweets_padded: np.ndarray = padTweets(test_tweets_index)\n",
    "\n",
    "softmax_output: List[List[float]] = model_RNN.predict(test_tweets_padded)\n",
    "predicted_values: List[int] = []\n",
    "for scores in softmax_output:\n",
    "    highest_score_label: int = np.argmax(scores)\n",
    "    predicted_values.append(highest_score_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "confusion_matrix(test_labels, predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+\n",
    "evaluation.main(test_labels, predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-reasoning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
